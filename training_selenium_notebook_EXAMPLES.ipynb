{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Example implementations\n",
    "Below are the two realizations of selenium for scraping **Outlook** and **Skype**. The step by step structure is explained in the <a href=\"./adv04_selenium_skypeOutlook.pdf\" ><b>PDF Report<b></a>. This is just a demonstration of what full fledging scraping looks like\n",
    "\n",
    "<img src=\"images_inkscape/structure_selenium.png\" style=\"width: 500px;\">\n",
    "\n",
    "***\n",
    "#Selenium Class and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# create a browser instance\n",
    "from selenium import webdriver\n",
    "\n",
    "# emulate keyboard inputs\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# creatinga single browser instance\n",
    "import selenium.webdriver.firefox.service as service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "\n",
    "# WebDriverWait and EC to allow waiting for element to load on page\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# module to search for elements using xpaths\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "\n",
    "# exception handling\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "\n",
    "# quick clicking and scrolling\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "\n",
    "# searching of html with \"find()\"\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import sys\n",
    "import math\n",
    "import os                       # file saving\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class selenium_bot():\n",
    "    \"\"\"\n",
    "    Interactable bot, that parses outlook files\n",
    "    \"\"\"\n",
    "    def __init__(self, browser, timeout, save_period, url, succesful_login_xpath):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [str] browser: \"Firefox\" or \"Chrome\"\n",
    "        [float] timeout: how long to wait for responses from webpage\n",
    "        [save_period] float: time in seconds to create backup of parsed data\n",
    "        [str] url: url bot starts off at\n",
    "        [str] succesful_login_xpath: xpath to indicate that page has loaded\n",
    "\n",
    "        __ Description __\n",
    "        sets up selenium bot\n",
    "        \"\"\"\n",
    "\n",
    "        self.browser = browser.lower()\n",
    "        self.timeout = timeout\n",
    "        self.url = url\n",
    "        self.succesful_login_xpath = succesful_login_xpath\n",
    "        \n",
    "        # 1 - setup browser\n",
    "        self.driver = self.__setup_chrome()\n",
    "        self.driver.maximize_window()\n",
    "\n",
    "        # 2 - load page\n",
    "        self.driver.get(self.url)\n",
    "\n",
    "        # 3- supprorting parameters for the future\n",
    "        # waiter, to wait for contents to load. call the \"waiter.until(function)\" method\n",
    "        self.WebDriverWaiter = WebDriverWait(self.driver, self.timeout)\n",
    "        self.save_period = save_period\n",
    "        \n",
    "        print(\"==> setup_browser end\\n\")\n",
    "\n",
    "    def __setup_firefox(self):\n",
    "        \"\"\"\n",
    "        __ Description __\n",
    "        open up a firefox driver\n",
    "\n",
    "        __ Returns __\n",
    "        driver handle\n",
    "        \"\"\"\n",
    "\n",
    "        # 1 - create a browser instance\n",
    "        print(\"  > Starting new Firefox server\")\n",
    "        browser = webdriver.Firefox(\n",
    "            executable_path='./geckodriver')\n",
    "\n",
    "        return browser\n",
    "\n",
    "    def __setup_chrome(self):\n",
    "        \"\"\"\n",
    "        __ Description __\n",
    "        open up a chrome driver\n",
    "\n",
    "        __ Returns __\n",
    "        driver handle\n",
    "        \"\"\"\n",
    "\n",
    "        # 1 - set capabilities\n",
    "        capabilities = {'chromeOptions':\n",
    "                        {\n",
    "                            'useAutomationExtension': False,\n",
    "                            'args': ['--disable-extensions']}\n",
    "                        }\n",
    "\n",
    "        # 2 - set options for chrome\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_experimental_option(\"prefs\", {\n",
    "            \"download.prompt_for_download\": False,\n",
    "            \"download.directory_upgrade\": True,\n",
    "            \"safebrowsing.enabled\": True\n",
    "        })\n",
    "\n",
    "        # 3 - create a browser instance with defined options\n",
    "        print(\"  > Starting new Chrome server\")\n",
    "        browser = webdriver.Chrome(executable_path=\"./chromedriver\",\n",
    "                                   desired_capabilities=capabilities,\n",
    "                                   options=chrome_options)\n",
    "        return browser\n",
    "    \n",
    "    def supp_extract_html(self, soup, html_tags_array):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [soup] soup: html to extract from formatted with BeautifulSoup\n",
    "        [arr] html_tags_array: array of the form\n",
    "        \n",
    "                                    [[\"div\", {\"role\": \"option\"}], \n",
    "                                    [\"div\", {\"aria-label\": \"Reading Pane\"}], \n",
    "                                    ...]\n",
    "\n",
    "        which specifies the name (\"div\", \"span\") and attributes ({\"id\": [\"test1\", \"test2\"], \"aria-label\": \"pane\"})\n",
    "        from outer to inner tags, iteratively going down specificity levels\n",
    "\n",
    "        __ Description __\n",
    "        iterates through the supplied \"soup\" html looking for tags whose parrents match all the supplied \"html_tags\"\n",
    "\n",
    "        __ Return __\n",
    "        [htmltag1, htmltag2, htmltag3]: array of html tags that fit the search requirement\n",
    "        \"\"\"\n",
    "\n",
    "        structure_depth  = len(html_tags_array)\n",
    "        debug_counter = 0\n",
    "\n",
    "        try:\n",
    "            if(structure_depth != 1):\n",
    "                # 1 - unpack the first structure\n",
    "                current_structure = soup.find(\n",
    "                    html_tags_array[0][0], attrs=html_tags_array[0][1])\n",
    "\n",
    "                # 2 - unpack further structures until we get to the last one\n",
    "                for i in range(1, structure_depth - 1):\n",
    "                    debug_counter += 1\n",
    "                    name = html_tags_array[i][0]\n",
    "                    attrs = html_tags_array[i][1]\n",
    "                    current_structure = current_structure.find(names, attrs=attrs)\n",
    "                # 3 - extract all matches from the lowest structure\n",
    "                current_structure = current_structure.find_all(\n",
    "                    html_tags_array[-1][0], attrs=html_tags_array[-1][1])\n",
    "            else:\n",
    "                # 1 - in the special case that only one structure is specified\n",
    "                current_structure = soup.find_all(\n",
    "                    html_tags_array[0][0], attrs=html_tags_array[0][1])\n",
    "\n",
    "            return current_structure\n",
    "            \n",
    "        except AttributeError:\n",
    "            # Error when an entry is missing\n",
    "            print(\"The page does not have the html element:\\n\\t[%s, %s]\"\n",
    "                  % (html_tags_array[debug_counter], html_tags_array[debug_counter]))\n",
    "            \n",
    "            return \"\"\n",
    "        \n",
    "    def supp_extract_text(self, soup, html_tags_array):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [soup] soup: html to extract from formatted with BeautifulSoup\n",
    "        html_tags_array: array of the form\n",
    "        \n",
    "        [[\"div\", {\"role\": \"option\"}], \n",
    "        [\"div\", {\"aria-label\": \"Reading Pane\"}], \n",
    "        ...]\n",
    "\n",
    "        which specifies the name (\"div\", \"span\") and attributes ({\"id\": [\"test1\", \"test2\"], \"aria-label\": \"pane\"})\n",
    "        from outer to inner tags, iteratively going down specificity levels\n",
    "\n",
    "        __ Description __\n",
    "        iterates through the supplied \"soup\" html looking for tags whose parrents match all the supplied \"html_tags\"\n",
    "        then a text array is extracted from this tag\n",
    "\n",
    "        __ Return __\n",
    "        [array] matching text in the innter structure\n",
    "        \"\"\"\n",
    "\n",
    "        html_structure = self.supp_extract_html(soup, html_tags_array)\n",
    "        \n",
    "        # 1 - take all of the tags found and extract text\n",
    "        array_to_return = [i.get_text().strip() for i in html_structure]\n",
    "        \n",
    "        return array_to_return\n",
    "        \n",
    "    def supp_write_to_element(self, element_xpath, fill_value):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [str] element_xpath: element to look for e.g. //div[@id=|password|]\n",
    "        [str] fill_value: what to write in the form\n",
    "\n",
    "        __ Description __\n",
    "        enters the \"fill_value\" into the chosen \"element\"\n",
    "        \"\"\"\n",
    "        self.supp_wait_for_xpath(element_xpath, \"input_box\")\n",
    "        \n",
    "        element = self.driver.find_element_by_xpath(element_xpath)\n",
    "        if(element):\n",
    "            element.send_keys(fill_value)\n",
    "        else:\n",
    "            print(\"**> Element with xpath %s does not exist\" %element_xpath)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def supp_wait_for_xpath(self, xpath, description):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [str] xpath: xpath to wait for\n",
    "        [str] description: the object that is trying to be located. will be printed to console. \n",
    "                           \"NA\" to skip\n",
    "\n",
    "        __ Description __\n",
    "        pauses the browser until \"xpath\" is loaded on the page\n",
    "        \"\"\"\n",
    "\n",
    "        if(description != \"NA\"):\n",
    "            print(\"  > Waiting for \\\"%s\\\" to load\" %(description))\n",
    "            \n",
    "        self.WebDriverWaiter.until(\n",
    "            EC.presence_of_element_located(\n",
    "                (By.XPATH, xpath)), \n",
    "            message=\"Did not find %s within the timeout time you set of %i\"%(xpath, self.timeout)\n",
    "        )\n",
    "        \n",
    "    def supp_click(self, xpath):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [str] xpath: xpath of object to click\n",
    "\n",
    "        __ Description __\n",
    "        clicks the element\n",
    "        \"\"\"\n",
    "        print(self.driver.find_element_by_xpath(xpath))\n",
    "        self.driver.find_element_by_xpath(xpath).click()\n",
    "        \n",
    "    def supp_load_soup(self):\n",
    "        \"\"\"\n",
    "        Loads up a soup of all the html on the visible page\n",
    "        __ Returns __\n",
    "        Soup Object to search\n",
    "        \"\"\"\n",
    "        html = self.driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        return soup\n",
    "    \n",
    "    def refresh(self):\n",
    "        \"\"\"\n",
    "        __ Description __\n",
    "        Resets variables of bot class and reload page\n",
    "        \"\"\"\n",
    "\n",
    "        self.driver.get(self.url)\n",
    "        self.supp_wait_for_xpath(self.succesful_login_xpath, \"main page\")\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        __ Description __\n",
    "        clears the pandas_out array to the initial value\n",
    "        \"\"\"\n",
    "\n",
    "        self.pandas_scraped = pd.DataFrame(columns=self.pandas_columns)\n",
    "\n",
    "    def save_data(self, file_name=\"pandas_out\", ext=\"csv\"):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [str] file_name: the file to save to. provide .pkl or .csv extension\n",
    "        \n",
    "        __ Description __\n",
    "        Saves data accumulated in \"pandas_out\" to output file\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1 - create output directory\n",
    "        if not os.path.exists(\"./output\"):\n",
    "            os.mkdir(\"output\")\n",
    "\n",
    "        # 2 - cut any extensions that were given by accident\n",
    "        file_name = file_name.split(\".\")[0]\n",
    "        file_name = \"./output/%s\" % (file_name)\n",
    "        \n",
    "        if(ext == \"pkl\"):\n",
    "            self.pandas_scraped.to_pickle(\"%s.pkl\" % file_name)\n",
    "        else:\n",
    "            self.pandas_scraped.to_csv(\"%s.csv\" % file_name)\n",
    "\n",
    "    def date_from_string(self, date_string):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [str] date_string: either day of week or \"18 May 2019\"\n",
    "\n",
    "        __ Description __\n",
    "        convert to an array numerical date values. if a weekday was supplied, find the nearest previous date\n",
    "\n",
    "        __ Return __\n",
    "        [year, month, day] date: array of the date\n",
    "        \"\"\"\n",
    "\n",
    "        weekday_list = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\",\n",
    "                        \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
    "\n",
    "\n",
    "        if (date_string in weekday_list):\n",
    "            # 1 - set loop parameters\n",
    "            date = datetime.date.today()\n",
    "            date_shift = datetime.timedelta(days = 1)\n",
    "            date_found = False\n",
    "\n",
    "            # 2 - decrease date, until the weekday_list match\n",
    "            while(not date_found):\n",
    "                date = date - date_shift\n",
    "                day_of_the_week_long = weekday_list[date.weekday()]\n",
    "                day_of_the_week_short = weekday_list[date.weekday() + 7]\n",
    "                if((day_of_the_week_long == date_string) or (day_of_the_week_short == date_string)):\n",
    "                    date_found = True\n",
    "        else:\n",
    "            date = datetime.datetime.strptime(date_string, '%d %B %Y')\n",
    "\n",
    "        date_array = [date.year, date.month, date.day]\n",
    "        return date_array\n",
    "\n",
    "    def string_from_date(self, date_array):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [year, month, day] date: array of the date\n",
    "\n",
    "        __ Description __\n",
    "        converts the array to string representation \"18 May 2019\"\n",
    "\n",
    "        __ Return __\n",
    "        [str] date_string\n",
    "        \"\"\"\n",
    "\n",
    "        date = datetime.datetime(date_array[0], date_array[1], date_array[2])\n",
    "        return date.strftime(\"%d %B %Y\")    \n",
    "\n",
    "    def datetime_from_date(self, date_array):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [year, month, day] date: array of the date\n",
    "\n",
    "        __ Description __\n",
    "        converts the array to a datetime object\n",
    "\n",
    "        __ Return __\n",
    "        [datetime] datetimeObject\n",
    "        \"\"\"\n",
    "        return datetime.datetime(date_array[0], date_array[1], date_array[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# ðŸ“§ Outlook class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class outlook_bot(selenium_bot):\n",
    "    \"\"\"bot to extract email content from outlook\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, browser, timeout, save_period=5, url=\"https://mail.sinobestech.com.hk/owa\", succesful_login_xpath = \"//div[@class = 'flex flexcolumn']\"):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [str] browser: \"Firefox\" or \"Chrome\"\n",
    "        [float] timeout: how long to wait for tiemouts on the page\n",
    "        [int] save_period: during scraping of email, how often to save an output file. default every 5 emails\n",
    "\n",
    "        __ Description __\n",
    "        initialisation of web driver and outlook variables\n",
    "        \"\"\"\n",
    "\n",
    "        # 1 - setup driver\n",
    "        selenium_bot.__init__(self, browser, timeout, int(save_period), url, succesful_login_xpath)\n",
    "\n",
    "        # 2 - setup outlook environment\n",
    "        self.__setup_outlook()\n",
    "        \n",
    "    def __setup_outlook(self):\n",
    "        \"\"\"\n",
    "        __ Description __\n",
    "        Sets up supporting objects for outlook\n",
    "\n",
    "        self.pandas_scraped: ouput dataframe with keys:\n",
    "        [ \"From\", \"Date\", \"Subject\", \"Content_Conversation\", \"Content_Forwarded\"]\n",
    "        \"\"\"\n",
    "\n",
    "        # 1 - pandas dataframe\n",
    "        self.pandas_columns = [ \"From\", \"Date\", \"Subject\", \"Content_Conversation\", \"Content_Forwarded\"]\n",
    "        self.pandas_scraped = pd.DataFrame(columns=self.pandas_columns)\n",
    "\n",
    "        self.scrape_filters_set = False\n",
    "        \n",
    "        # 2 - debugging\n",
    "        self.entry_missing_array = [0] * 5\n",
    "        self.email_current = -1\n",
    "        self.email_total = -1\n",
    "\n",
    "    def outlook_login(self, outlook_id, password):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [str] outlook_id: email to log on with\n",
    "        [str] password:   password\n",
    "\n",
    "        __ Description __\n",
    "        logs into outlook\n",
    "        \"\"\"\n",
    "        print(\"==> outlook_login start\")\n",
    "        \n",
    "        # 1 - access the outlook page\n",
    "        self.supp_wait_for_xpath(\"//input[@id='username']\", \"user_name_input_field\")\n",
    "\n",
    "        # 2 - locate credential fields and fill them in\n",
    "        self.supp_write_to_element(\"//input[@id='username']\", outlook_id)\n",
    "        self.supp_write_to_element(\"//input[@id='password']\", password)\n",
    "        print(type(self.driver.find_element_by_xpath(\n",
    "            \"//div[@onclick='clkLgn()']\")))\n",
    "        self.driver.find_element_by_xpath(\n",
    "            \"//div[@onclick='clkLgn()']\").click()\n",
    "\n",
    "        # 3 - ensure that login is succesfull and wait for emails to load\n",
    "        self.supp_wait_for_xpath(self.succesful_login_xpath, \"main_page\")\n",
    "\n",
    "        print(\"==> outlook_login end\\n\")\n",
    "\n",
    "    def outlook_scrape_setup(self, date_min, date_max, only_unread=False, scan_min=0, scan_max=9999):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [year, month, day] date_min/date_max:   date range to scrape\n",
    "        [bool] only_unread:                     whether only unread emails should be scraped\n",
    "        [int] scan_min/max:                     email range to scrape\n",
    "\n",
    "        __ Description __\n",
    "        sets values in preparation for scraping outlook\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"==> outlook_scrape_setup start\")\n",
    "\n",
    "        self.criteria = {}\n",
    "\n",
    "        # 1 - set dates if supplied\n",
    "        if(date_min):\n",
    "            self.criteria['date_min'] = date_min\n",
    "            print(\"  > Minimum date:\\t\",\n",
    "                  datetime.datetime(date_min[0], date_min[1], date_min[2]).strftime(\"%A, %d %b %Y\"))\n",
    "        else:\n",
    "            self.criteria['date_min'] = [1, 1, 1]\n",
    "            print(\"  > No minimum date\")\n",
    "\n",
    "        if(date_max):\n",
    "            self.criteria['date_max'] = date_max\n",
    "            print(\"  > Maximal date:\\t\",\n",
    "                  datetime.datetime(date_max[0], date_max[1], date_max[2]).strftime(\"%A, %d %b %Y\"))\n",
    "        else:\n",
    "            self.criteria['date_max'] = [8888, 1, 1]  # highest possible date\n",
    "            print(\"  > No maximal date\")\n",
    "\n",
    "        # 2 - read/unread\n",
    "        self.criteria['only_unread'] = only_unread\n",
    "        if(only_unread):\n",
    "            print(\"  > Scraping only unread emails\")\n",
    "\n",
    "        # 3 -  min\n",
    "        self.criteria['scan_min'] = scan_min\n",
    "        self.criteria['scan_max'] = scan_max\n",
    "        print(f\"  > Email index start:\\t{scan_min}\\n  > Email index end:\\t{scan_max}\")\n",
    "\n",
    "        self.scrape_filters_set = True\n",
    "\n",
    "        print(\"==> outlook_scrape_setup end\\n\")\n",
    "\n",
    "        \n",
    "    def outlook_scrape(self, file_name=\"pandas_out\", ext=\"csv\"):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [str] filename:         to which to save dataframe\n",
    "        [str] ext:              format to save as - pkl or csv\n",
    "\n",
    "        __ Description __\n",
    "        Iterates through the emails and parses out information into pandas dataframe\n",
    "        \"\"\"\n",
    "        ########################################xpaths\n",
    "        inbox_mail_L1_xp = \"//div[@class = 'flex flexcolumn']\"\n",
    "        inbox_mail_L2_xp = \"//div[@role ='option']\"\n",
    "        inbox_mail_soup = [[\"div\", {\"class\": \"flex flexcolumn\"}],\n",
    "                           [\"div\", {\"role\": \"option\"}]]\n",
    "        ########################################\n",
    "        \n",
    "        print(\"==> outlook_scrape start\")\n",
    "            \n",
    "        # 0 - prepare variable\n",
    "        self.refresh()\n",
    "        self.supp_wait_for_xpath(self.succesful_login_xpath, \"NA\")\n",
    "\n",
    "        if(not self.scrape_filters_set):\n",
    "            self.outlook_scrape_setup(None, None)\n",
    "        inbox_cycle = True\n",
    "        email_loop_no = 0\n",
    "        email_base_index = 0            # base index is required to stitch email numbers across different loops\n",
    "        uniqueID_already_scraped = set()\n",
    "\n",
    "        while(inbox_cycle):\n",
    "\n",
    "            # 1 - [XPATH] extract unique tag and webelement of each email\n",
    "            visible_webElements = self.driver.find_element_by_xpath(inbox_mail_L1_xp).find_elements_by_xpath(inbox_mail_L2_xp)\n",
    "            visible_uniqueID = [i.text for i in visible_webElements]\n",
    "\n",
    "            # 2 - [SOUP] extract metadata of visible mail\n",
    "            soup = self.supp_load_soup()\n",
    "            visible_metadataRaw = self.supp_extract_html(soup, inbox_mail_soup)\n",
    "            visible_metadata = []\n",
    "            for i in visible_metadataRaw:\n",
    "                visible_metadata.append({\"date\": self.outlook_inbox_date(i),\n",
    "                                         \"unread\": self.outlook_inbox_unread(i)})\n",
    "\n",
    "            # 3 - only iterate through unscraped mail i.e. uniqueID is not in \"uniqueID_already_scraped\" set\n",
    "            emails_to_scrape = []\n",
    "            i = 0\n",
    "            for email_webElement, email_uniqueID, email_metadata in zip(visible_webElements, visible_uniqueID, visible_metadata):\n",
    "                if(email_uniqueID not in uniqueID_already_scraped):\n",
    "                    \n",
    "                    # 3a - add the email number\n",
    "                    email_metadata[\"email_no\"] = email_base_index + i\n",
    "                    i += 1\n",
    "                    # 3b - store email id and email_metadata for further extraction\n",
    "                    emails_to_scrape.append({\"email_webElement\": email_webElement,\n",
    "                                            \"email_metadata\": email_metadata})\n",
    "                    # 3c - store the unqiue tag to prevent scraping it in the future\n",
    "                    uniqueID_already_scraped.add(email_uniqueID)\n",
    "\n",
    "\n",
    "            print(\"  [Loop No.%i]:\\t%i unique emails found in inbox so far\" % (email_loop_no, len(uniqueID_already_scraped)))\n",
    "\n",
    "            # 4 - iterate through only the new emails\n",
    "            for i, email in enumerate(emails_to_scrape):\n",
    "                \n",
    "                # a - check that email_metadata fulfills criteria\n",
    "                criteria_satisfied = self.criteria_check(email['email_metadata'],\n",
    "                                                         self.criteria)\n",
    "\n",
    "                if(criteria_satisfied):\n",
    "                    print(self.outlook_scrape_print_progress(email['email_metadata']))\n",
    "                    email_content = self.outlook_scrape_email(email['email_webElement'])\n",
    "                    self.pandas_scraped = self.pandas_scraped.append(email_content, ignore_index=True)\n",
    "                    \n",
    "                    # b - write to file periodically during intense data writting\n",
    "                    if(((email['email_metadata']['email_no'\n",
    "                    ] - self.criteria['scan_min'] + 1) % self.save_period) == 0):\n",
    "                        self.save_data(file_name, ext)\n",
    "                    \n",
    "            # 5 - click on last mail (to scroll down)\n",
    "            self.outlook_scrape_email(emails_to_scrape[-1]['email_webElement'])\n",
    "            \n",
    "            # a - set variables for next loop\n",
    "            email_loop_no += 1\n",
    "            email_base_index = len(uniqueID_already_scraped)\n",
    "\n",
    "            # b - check against max emails scraped\n",
    "            if(email_base_index > self.criteria['scan_max']):\n",
    "                # if we have scraped all the emails, stop\n",
    "                inbox_cycle = False\n",
    "\n",
    "            # c - check against no scrolling (same emails displayed)\n",
    "            visible_idx_new = self.driver.find_element_by_xpath(inbox_mail_L1_xp).find_elements_by_xpath(inbox_mail_L2_xp)\n",
    "            if (visible_webElements == visible_idx_new):\n",
    "                inbox_cycle = False\n",
    "\n",
    "        # 5 - save data and exit\n",
    "        print(\"==> Scraped %i emails that fit the criteria\" %(len(uniqueID_already_scraped)))        \n",
    "        self.save_data(file_name, ext)        \n",
    "        print(\"==> outlook_scrape end\")\n",
    "\n",
    "    def outlook_scrape_print_progress(self, email_metadata):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [dict] email_metadata:          {\"date\":        [year, month, day] of email,\n",
    "                                         \"unread\":      [bool] read status of email,\n",
    "                                         \"email_no\":    [int] top email is 0}\n",
    "\n",
    "        __ Description __\n",
    "        generates string to print to console about the emial currently being extracted\n",
    "\n",
    "        __ Returns __\n",
    "        [str] string to print to console describing email being extracted\n",
    "        \"\"\"\n",
    "        email_no = email_metadata['email_no']\n",
    "        date = email_metadata['date']\n",
    "        unread = email_metadata['unread']\n",
    "\n",
    "        # 1 - read undread\n",
    "        string_read = \"\\t[Read]\"\n",
    "        if(unread):\n",
    "            string_read = \"\\t[Unread]\"\n",
    "            \n",
    "        # 2 - date\n",
    "        string_date = \"\\t[\" + datetime.datetime(date[0], date[1], date[2]).strftime(\"%A, %d %b %Y\") + \"]\"\n",
    "\n",
    "        string_to_print = f\"  > Scraping Email {email_no}\" + string_read + string_date\n",
    "\n",
    "        return string_to_print\n",
    "        \n",
    "    def outlook_scrape_email(self, email_webElement):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [web_element] email_webElement:            element found with xPath  \"self.driver.find_element_by_xpath(...)\"\n",
    "\n",
    "        __ Description __\n",
    "        extacts data from the given email (passed as a web_element)\n",
    "        \n",
    "        __ Return __\n",
    "        [dict]          {\"From\": e_from,\n",
    "                        \"Date\": e_date,\n",
    "                        \"Subject\": e_subject,\n",
    "                        \"Content_Conversation\": e_content_conversation,\n",
    "                        \"Content_Forwarded\": e_content_forwarded}\n",
    "        \"\"\"\n",
    "        # 1 - load up the email_webElement and wait for for load\n",
    "        try:\n",
    "            email_webElement.click()\n",
    "            self.supp_wait_for_xpath(\"//div[@id = 'Item.MessageUniqueBody']\", \"NA\")\n",
    "            self.WebDriverWaiter.until(wait_for_content_forwarded())\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(\n",
    "                \"**> Email failed to load. Increase timeout (currently %.1fs)\" % (self.timeout))\n",
    "            return\n",
    "\n",
    "        # 2 - extract html on the page. soup is the chad way to search this html\n",
    "        soup = self.supp_load_soup()\n",
    "\n",
    "        # a - subject\n",
    "        self.entry_current = 0\n",
    "        e_subject = \"\".join(self.supp_extract_text(soup,\n",
    "                                     [[\"div\", {\"aria-label\": \"Reading Pane\"} ],\n",
    "                                      [\"div\", {'role': \"heading\", \"aria-level\": \"2\"}]]))\n",
    "        \n",
    "        # b - from\n",
    "        self.entry_current = 1\n",
    "        e_from = \"\".join(self.supp_extract_text(soup,\n",
    "                                        [[\"div\", {\"aria-label\": \"Persona card\"} ]]))\n",
    "        match_groups = re.search(\"([^<]*)(.*)?\", e_from) # remove email_webElement <ilya.antonv....>\n",
    "        e_from = match_groups.group(1).strip()\n",
    "\n",
    "        # c - date and time\n",
    "        self.entry_current = 2\n",
    "        e_date = \"\".join(self.supp_extract_text(soup,\n",
    "                                         [[\"div\", {\"class\": \"_rp_f8\"}],\n",
    "                                          [\"span\", {\"class\": \"allowTextSelection\"} ]]))\n",
    "\n",
    "        # d - email_webElement content\n",
    "        self.entry_current = 3\n",
    "        e_content_conversation = \"\".join(self.supp_extract_text(soup,\n",
    "                                            [[\"div\", {\"aria-label\": \"Reading Pane\"}],\n",
    "                                             [\"div\", {\"role\": \"document\"}]]))\n",
    "\n",
    "        self.entry_current = 4\n",
    "        e_content_forwarded = \"\".join(self.supp_extract_text(soup,\n",
    "                                         [[\"div\", {\"aria-label\": \"Reading Pane\"}],\n",
    "                                          [\"div\", {\"id\": \"Conversation.FossilizedTextBody\"}]]))\n",
    "\n",
    "        #print(\"_______Subject________\\n%s\\n\" %e_subject)\n",
    "        #print(\"_______From________\\n%s\\n\" %e_from)        \n",
    "        #print(\"_______Date________\\n%s\\n\" %e_date)        \n",
    "        #print(\"_______Content_Conversation________\\n%s\\n\" %e_content_conversation)        \n",
    "        #print(\"_______Content_Forwarded________\\n%s\\n\" %e_content_forwarded)\n",
    "        \n",
    "        # 6 - structure building and return\n",
    "        email_entry = {\"From\": e_from,\n",
    "                       \"Date\": e_date,\n",
    "                       \"Subject\": e_subject,\n",
    "                       \"Content_Conversation\": e_content_conversation,\n",
    "                       \"Content_Forwarded\": e_content_forwarded}\n",
    "        \n",
    "        return email_entry\n",
    "    \n",
    "    def outlook_inbox_date(self, inbox_tag):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [soup] inbox_tag: a html tag of an particular email in the inbox\n",
    "\n",
    "        __ Description __\n",
    "        extracts a date of the email in the inbox column by searching the \"inbox_tag\"\n",
    "\n",
    "        __ Returns __\n",
    "        [day, month, year]\n",
    "        \"\"\"\n",
    "\n",
    "        ########################################\n",
    "        date_attr = {\"class\": [\"_lvv_M\"]}\n",
    "        ########################################\n",
    "\n",
    "        # 1 - extract date tag\n",
    "        date_tag = inbox_tag.find(attrs=date_attr)\n",
    "        date_inbox = date_tag.get_text()\n",
    "\n",
    "        # 2 - split date put by slashes. this will work for old entries\n",
    "        date_return = date_inbox.split(\"/\")\n",
    "        date_return = date_return[::-1] # reverse order so that [year, month, day]\n",
    "        \n",
    "        if(len(date_return) != 3):\n",
    "            # 3 - for email sent this week, the first string is the day of the week, which is converted to [year, month, day]\n",
    "            weekday = date_inbox.split(\" \")[0]\n",
    "            date_return = self.date_from_string(weekday)\n",
    "\n",
    "        # 3 - convert to int\n",
    "        date_return = [int(i) for i in date_return]\n",
    "            \n",
    "        return date_return\n",
    "\n",
    "    def outlook_inbox_unread(self, inbox_tag):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [soup] inbox_tag: a html tag of an particular email in the inbox\n",
    "\n",
    "        __ Description __\n",
    "        checks if email unread or not\n",
    "\n",
    "        __ Returns __\n",
    "        True if unread. False otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        # 1 - read email have a \"_lvv_y_\" tag\n",
    "        mg = re.search(\"\\s_lvv_y\\s\", str(inbox_tag))\n",
    "\n",
    "        # 2 - check if match was found, indicating that email has been read\n",
    "        if(mg):\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    def criteria_check(self, email_metadata, criteria):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [dict] email_metadata:  {'email_no':   \t[int]  email number as it appears in inbox\n",
    "                                 'date':       \t[year, month, day] of email\n",
    "                                 'unread':     \t[bool]  read status of email}\n",
    "\n",
    "        [dict] criteria:        {'scan_min':   [int] range to scrape (0 for top email)\n",
    "                                 'scan_max':\n",
    "                                 'date_min':    [int] date range to scrape\n",
    "                                 'data_max':\n",
    "                                 'only_unread': [bool] whether to scrape only unread emails}\n",
    "\n",
    "        __ Description __\n",
    "        checks whether email should be scraped based off it's email_metadata\n",
    "\n",
    "        __ Return __\n",
    "        True/False\n",
    "        \"\"\"\n",
    "        # 1 - extract criteria info\n",
    "        date_min = criteria['date_min']\n",
    "        date_max = criteria['date_max']\n",
    "        date_min = datetime.datetime(date_min[0], date_min[1], date_min[2])\n",
    "        date_max = datetime.datetime(date_max[0], date_max[1], date_max[2])\n",
    "\n",
    "        # 2 - extract email_metadata\n",
    "        unread = email_metadata['unread']\n",
    "        email_no = email_metadata['email_no']\n",
    "        date = email_metadata['date']\n",
    "        date = datetime.datetime(date[0],date[1],date[2])    \n",
    "\n",
    "        return_val = False\n",
    "\n",
    "        ########################################\n",
    "        # â¦¿ Perform check\n",
    "        ########################################\n",
    "        # 1 - check that email is within indicies\n",
    "        if((criteria['scan_min'] <= email_no) and (email_no <= criteria['scan_max'])):\n",
    "            \n",
    "            # 2 - check date\n",
    "            if ((date_min <= date) and (date <= date_max)):\n",
    "                \n",
    "                # 3 - if scraping only unread, check unread status\n",
    "                if(criteria['only_unread']):\n",
    "                    if(unread):\n",
    "                        return_val = True\n",
    "                    else:\n",
    "                        return_val = False\n",
    "                else:\n",
    "                    return_val = True\n",
    "\n",
    "        return return_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wait_for_content_forwarded():\n",
    "  \"\"\"Checking that IF there is a forwarded message, that it has been loaded\n",
    "\n",
    "  returns True if there is no forwarding message or it has been loaded\n",
    "\n",
    "  To be used in the following way:\n",
    "  formWebDriverWait.until(wait_for_content_forwarded())\n",
    "  \"\"\"\n",
    "\n",
    "  def __call__(self, driver):\n",
    "    \"\"\"\n",
    "    __ Parameters __\n",
    "    driver: the WebDriverWait.until(xxx) calls method xxx with 'driver' as the first \n",
    "    argument.\n",
    "\n",
    "    __ Description __\n",
    "    ensure that any forwarded email is fully loaded\n",
    "\n",
    "    a forwarded email has a non empty <div of forwarded email> in the following positon:\n",
    "\n",
    "    <div aria-label='Reading-Pane> ..... \n",
    "        <div>....</div>\n",
    "        <div>        <---------- div[2]\n",
    "            <div>...</div>\n",
    "            etc. etc.\n",
    "            <div of forwarded email> <----------- NON empty when there is forwarding\n",
    "            <div>...</div>        <---------- div[last()]\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    __ Return __\n",
    "    True: if forwarded email loaded\n",
    "    False: if forwaded email has NOT loaded\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1 - test if there is a forwarded section, by checking that the <div of forwarded email> is not empty\n",
    "    try:\n",
    "        driver.find_element_by_xpath(\"//div[@aria-label='Reading Pane']/div[2]/div[last()-1]/*\")\n",
    "    except NoSuchElementException:\n",
    "        #  if no email is being forwarded then we don't have to wait\n",
    "        return True\n",
    "\n",
    "    # 2 - IF there is a forwarded email, wait for the body of the forwarded email to load\n",
    "    try:\n",
    "        driver.find_element_by_xpath(\"//div[@id='Conversation.FossilizedTextBody']/div[1]\")\n",
    "        return True\n",
    "    except NoSuchElementException:\n",
    "        #  treurn flase if the email has not loaded yet\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  > Starting new Chrome server\n",
      "==> setup_browser end\n",
      "\n",
      "==> outlook_login start\n",
      "  > Waiting for \"user_name_input_field\" to load\n",
      "  > Waiting for \"input_box\" to load\n",
      "  > Waiting for \"input_box\" to load\n",
      "<class 'selenium.webdriver.remote.webelement.WebElement'>\n",
      "  > Waiting for \"main_page\" to load\n",
      "==> outlook_login end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "########################################\n",
    "outlook_id=\"programmer01@sbtgc.local\"\n",
    "password=\"3Zwl26EiY\"\n",
    "timeout=50                      # seconds to wait for page elements to load before quitting\n",
    "browser=\"chrome\"                # firefox of chrome\n",
    "########################################\n",
    "########################################\n",
    "outlook_class = outlook_bot(browser, timeout)\n",
    "outlook_class.outlook_login(outlook_id, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> outlook_scrape_setup start\n",
      "  > No minimum date\n",
      "  > Maximal date:\t Saturday, 25 May 2019\n",
      "  > Email index start:\t0\n",
      "  > Email index end:\t1000\n",
      "==> outlook_scrape_setup end\n",
      "\n",
      "==> outlook_scrape start\n",
      "  > Waiting for \"main page\" to load\n",
      "  [Loop No.0]:\t25 unique emails found in inbox so far\n",
      "  > Scraping Email 0\t[Read]\t[Friday, 12 Apr 2019]\n",
      "  > Scraping Email 1\t[Read]\t[Friday, 12 Apr 2019]\n",
      "  > Scraping Email 2\t[Read]\t[Friday, 12 Apr 2019]\n",
      "  > Scraping Email 3\t[Read]\t[Friday, 12 Apr 2019]\n",
      "  > Scraping Email 4\t[Read]\t[Friday, 12 Apr 2019]\n",
      "  > Scraping Email 5\t[Read]\t[Friday, 12 Apr 2019]\n",
      "  > Scraping Email 6\t[Read]\t[Friday, 12 Apr 2019]\n",
      "  > Scraping Email 7\t[Read]\t[Friday, 12 Apr 2019]\n",
      "  > Scraping Email 8\t[Read]\t[Friday, 12 Apr 2019]\n",
      "  > Scraping Email 9\t[Read]\t[Friday, 12 Apr 2019]\n",
      "  > Scraping Email 10\t[Read]\t[Friday, 12 Apr 2019]\n",
      "  > Scraping Email 11\t[Read]\t[Friday, 12 Apr 2019]\n",
      "  > Scraping Email 12\t[Read]\t[Friday, 12 Apr 2019]\n",
      "  > Scraping Email 13\t[Read]\t[Friday, 12 Apr 2019]\n",
      "  > Scraping Email 14\t[Read]\t[Friday, 12 Apr 2019]\n",
      "  > Scraping Email 15\t[Read]\t[Friday, 12 Apr 2019]\n",
      "  > Scraping Email 16\t[Read]\t[Friday, 12 Apr 2019]\n",
      "  > Scraping Email 17\t[Read]\t[Friday, 12 Apr 2019]\n",
      "  > Scraping Email 18\t[Read]\t[Friday, 12 Apr 2019]\n",
      "  > Scraping Email 19\t[Read]\t[Friday, 12 Apr 2019]\n",
      "  > Scraping Email 20\t[Read]\t[Friday, 12 Apr 2019]\n",
      "  > Scraping Email 21\t[Read]\t[Friday, 12 Apr 2019]\n",
      "  > Scraping Email 22\t[Read]\t[Friday, 12 Apr 2019]\n",
      "  > Scraping Email 23\t[Read]\t[Friday, 12 Apr 2019]\n",
      "  > Scraping Email 24\t[Read]\t[Friday, 12 Apr 2019]\n",
      "  [Loop No.1]:\t27 unique emails found in inbox so far\n",
      "  > Scraping Email 25\t[Read]\t[Friday, 12 Apr 2019]\n",
      "  > Scraping Email 26\t[Read]\t[Friday, 12 Apr 2019]\n",
      "==> Scraped 27 emails that fit the criteria\n",
      "==> outlook_scrape end\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "########################################\n",
    "unread_only = False\n",
    "date_min = None                 # either None or [2018, 1, 1]\n",
    "date_max = [2019, 5, 25]        # either None or [2018, 1, 1]\n",
    "# optional arguments (can call scrape_filters without them)\n",
    "# top email has an id=0, second email id=1 etc.\n",
    "id_min = 0                      # set 0 to include all emails\n",
    "id_max = 1000                   # set to 1000 to include all emails\n",
    "########################################\n",
    "########################################\n",
    "outlook_class.outlook_scrape_setup(date_min, date_max, unread_only, id_min, id_max)\n",
    "outlook_class.outlook_scrape(\"outlook\",\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# â˜Ž Skype class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class skype_bot(selenium_bot):\n",
    "    \"\"\"bot to extract email content from skpye\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, browser, timeout, save_period=5,\n",
    "                 url=\"https://web.skype.com\",\n",
    "                 succesful_login_xpath=\"//div[@role='group'][@aria-label='Conversations list']\"):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [str] browser:                  \"Firefox\" or \"Chrome\"\n",
    "        [float] timeout:                how long to wait for tiemouts on the page\n",
    "        [str] url:                      of page to visit\n",
    "        [str] succesful_login_xpath:    xpath to indicate that page has loaded\n",
    "\n",
    "        __ Description __\n",
    "        initialisation of web driver and skype variables\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # 1 - setup driver\n",
    "            selenium_bot.__init__(self, browser, timeout, int(save_period), url, succesful_login_xpath)\n",
    "\n",
    "            # 2 - setup skype environment\n",
    "            self.__setup()\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(\n",
    "                \"**> Page failed to fully load. Increase timeout (currently %.1fs)\" % (self.timeout))\n",
    "            return\n",
    "        \n",
    "    def __setup(self):\n",
    "        \"\"\"\n",
    "        __ Description __\n",
    "        Sets up supporting objects for skype\n",
    "\n",
    "        self.pandas_scraped: ouput dataframe with keys:\n",
    "        [ \"From\", \"Date\", \"Message\"]\n",
    "        \"\"\"\n",
    "\n",
    "        # 1 - pandas dataframe\n",
    "        self.pandas_columns = [ \"From\", \"Date\", \"Message\"]\n",
    "        self.pandas_scraped = pd.DataFrame(columns=self.pandas_columns)\n",
    "\n",
    "        self.scrape_filters_set = False\n",
    "\n",
    "    def login(self, skype_id, password):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [str] skype_id:         email to log on with\n",
    "        [str] password:         password\n",
    "\n",
    "        __ Description __\n",
    "        logs into skype\n",
    "        \"\"\"\n",
    "        print(\"==> login start\")\n",
    "        ########################################xpaths\n",
    "        skype_login_box_xp = \"//input[@type='email']\"\n",
    "        skype_password_box_xp = \"//input[@type='password']\"\n",
    "        skype_submit_button_xp = \"//input[@id='idSIButton9']\"\n",
    "        skype_got_it_xp = \"//div[@data-text-as-pseudo-element='Got it!']\"\n",
    "        ########################################\n",
    "        \n",
    "        # 1 - wait for email box\n",
    "        self.supp_write_to_element(skype_login_box_xp, skype_id)\n",
    "        self.driver.find_element_by_xpath(skype_submit_button_xp).click()\n",
    "        time.sleep(3)           # <---------------------------------------- need to wait for password box to come up\n",
    "        # 2 - wait for password\n",
    "        self.supp_write_to_element(skype_password_box_xp, password)\n",
    "        self.driver.find_element_by_xpath(skype_submit_button_xp).submit()\n",
    "        \n",
    "        # 3 - remove popups after page has loaded\n",
    "        self.supp_wait_for_xpath(self.succesful_login_xpath, \"page\")\n",
    "        time.sleep(2)           # <---------------------------------------- wait for the popup box to come up\n",
    "        try:\n",
    "            self.driver.find_element_by_xpath(skype_got_it_xp).click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        print(\"==> login end\\n\")\n",
    "\n",
    "        \n",
    "    def skype_scrollCheck_date(self, critetia, current_values):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [dict] critetia:       {\"chats_to_scrape\":      [1D-int] starting from 0,             \n",
    "                                \"date_min\":             [year,month,day] \n",
    "                                \"date_max\":             [year,month,day] \n",
    "                                \"max_number_of_stalls\": [int] before continuing}\n",
    "        [dict] current_values  {\"current_date\":         [year,month,dayy],\n",
    "                                \"current_number_of_stalls\": [int]}\n",
    "\n",
    "        __ Description __\n",
    "        checks if the filter-defined date has been reached, to determine if scrolling should continue\n",
    "\n",
    "        __ Return __\n",
    "        True if scrolling should continue\n",
    "        False if it should be stopped\n",
    "        \"\"\"\n",
    "\n",
    "        criteria_min_date = criteria['min_date']\n",
    "        current_date = current_values['current_date']\n",
    "        \n",
    "        return_val = False\n",
    "\n",
    "        if(current_values['current_number_of_stalls'] < criteria['max_number_of_stalls']):\n",
    "            # 1 - continue scrolling if date is not defined\n",
    "            if(current_date == \"Undefined\"):\n",
    "                return True\n",
    "        \n",
    "            # 2 - compare the date reached so far in the chat with the filter date\n",
    "            current_date = self.date_from_string(current_date) # convert the date from string to array\n",
    "            current_date = datetime.datetime(current_date[0], current_date[1], current_date[2]) # initialie a datetime object\n",
    "            criteria_min_date = datetime.datetime(criteria_min_date[0], criteria_min_date[1], criteria_min_date[2])\n",
    "    \n",
    "            if (criteria_min_date <= current_date):\n",
    "                return_val = True\n",
    "                \n",
    "            else:\n",
    "                print(\"\\n  > Chat scrolled past the user-defined date: %s [now at %s]\"\n",
    "                      %(criteria_min_date.strftime(\"%d %B %Y\"), current_date.strftime(\"%d %B %Y\")))\n",
    "                print(\"  > Stopping scrolling of chat\")\n",
    "        else:\n",
    "            print(\"\\n  > Chat stalled for the maximal user-defined number of scrolls: %i\"\n",
    "                  %(criteria['max_number_of_stalls']))\n",
    "            print(\"  > Stopping scrolling of chat\")\n",
    "\n",
    "        return return_val\n",
    "   \n",
    "    def skype_scrape_setup(self, chats_to_scrape, max_number_of_stalls, date_min, date_max):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [1D-int] chats_to_scrape:       chat indexes in the inbox to scrape, 0 for top chat\n",
    "        [int] max_number_of_stalls:     during scrapping, scrolling leads to occasional pauses \n",
    "                                        while the earlier content is loaded. during this time \n",
    "                                        the chat page does not change. this specifies how many \n",
    "                                        times to wait when this happens before exiting\n",
    "        [2018, 02, 01] date_min/max:    date range to scrape for each chat\n",
    "\n",
    "        __ Description __\n",
    "        Initializes the list \"self.scrape_filters\" used by the scraping functions\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"==> skype_scrape_setup start\")\n",
    "        self.criteria = {}\n",
    "\n",
    "        # 1 - chats to scrape\n",
    "        self.criteria['chats_to_scrape'] = chats_to_scrape\n",
    "\n",
    "        # 2 - number of stalls\n",
    "        print(\"  > Maximal number of stalls:\\t %i\" %(max_number_of_stalls))\n",
    "        self.criteria['max_number_of_stalls'] = max_number_of_stalls\n",
    "        \n",
    "        # 3 - set date if supplied\n",
    "        if(date_min):\n",
    "            self.criteria['date_min'] = date_min\n",
    "            print(\"  > Minimum date:\\t\\t\",\n",
    "                  datetime.datetime(date_min[0], date_min[1], date_min[2]).strftime(\"%A, %d %b %Y\"))\n",
    "        else:\n",
    "            self.criteria['date_min'] = [1, 1, 1]  # lowest date\n",
    "            print(\"  > No minimum date\")\n",
    "\n",
    "        if(date_max):\n",
    "            self.criteria['date_max'] = date_max\n",
    "            print(\"  > Maximal date:\\t\\t\",\n",
    "                  datetime.datetime(date_max[0], date_max[1], date_max[2]).strftime(\"%A, %d %b %Y\"))\n",
    "        else:\n",
    "            self.criteria['date_min'] = [8888, 1, 1]  # highest possible date\n",
    "            print(\"  > No maximal date\")\n",
    "        \n",
    "        self.scrape_filters_set = True\n",
    "\n",
    "        print(\"==> skype_scrape_setup end\\n\")\n",
    "\n",
    "    def skype_scrape(self, ext=\"csv\"):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [str] ext: format to save as. pkl or csv\n",
    "\n",
    "        __ Description __\n",
    "        Iterates through chats in Skype, saving individual date ordered (old->new) chats to files\n",
    "        \"\"\"\n",
    "        ######################################## XPATH of recent chats in the sidebar\n",
    "        chats_in_sidebar_xp = \"//div[@aria-label='Conversations list']/div/div[1]/div/div/*\"\n",
    "        chats_in_sidebar_sender_xp = \"div/div/div[2]/div[1]/div\"\n",
    "        ########################################\n",
    "        \n",
    "        print(\"==> skype_scrape start\")\n",
    "\n",
    "        # 1 - set default scraping filters of scraping the full first chat\n",
    "        if(not self.scrape_filters_set):\n",
    "            self.skype_scrape_setup([0], 100, None, None)\n",
    "\n",
    "        # 2 - extract all of the chats - go through the ones in the chats_to_scrape list\n",
    "        chats_to_scrape = self.criteria['chats_to_scrape']\n",
    "        chats = self.driver.find_elements_by_xpath(chats_in_sidebar_xp)\n",
    "        \n",
    "        for i, chat in enumerate(chats):\n",
    "            if(i in chats_to_scrape):\n",
    "                chats_to_scrape.remove(i)\n",
    "                \n",
    "                # 3 - get the sender\n",
    "                sender = chats[i].find_element_by_xpath(chats_in_sidebar_sender_xp).get_attribute(\"data-text-as-pseudo-element\")\n",
    "                # replace dots, @ and spaces with underscores\n",
    "                sender = sender.lower()\n",
    "                sender = re.sub(\"(\\.|@|\\s)\", \"_\", sender)\n",
    "                self.chat_info = [sender]\n",
    "                \n",
    "                # 4 - click on each chat and scrape the content\n",
    "                chats[i].click()\n",
    "                #print(f\"  > Scraping Chat No.{i}:\\t {sender} \\t [{self.string_from_date(self.criteria['date_min']} - {self.string_from_date(self.criteria['date_max']}]\"\"\n",
    "                chat_content = self.skype_scrape_chat()\n",
    "\n",
    "                # 5 - save to file\n",
    "                self.save_data(\"skype_%s\" %(sender), ext)\n",
    "        \n",
    "        if(len(chats_to_scrape) != 0):\n",
    "            skipped_chats = re.sub(\"\\[|\\]\", \"\", str(chats_to_scrape))\n",
    "            print(\"\\t*** Did not scrape non-existing chat(s): %s ***\" %(skipped_chats))\n",
    "        print(\"==> skype_scrape end\")\n",
    "\n",
    "    def skype_scrape_chat(self):\n",
    "        \"\"\"\n",
    "        __ Description __\n",
    "        scrolls the skype chat, until the \"scrape_filters\" are satisfied e.g. reach 20th May 2018\n",
    "\n",
    "        while scrolling, extract all the messages visible in the chat, avoiding duplicates\n",
    "        \"\"\"\n",
    "\n",
    "        ################################## XPATH of chat\n",
    "        messages_xp = \"//div[@style='position: relative; display: flex; flex-direction: row; flex-grow: 1; flex-shrink: 1; overflow: hidden; align-items: stretch; background-color: rgb(255, 255, 255);']/div/div[2]/div/div/div/div/div/div/div/div/div/div/div/div/div/div[2]/div[@role='region']\"\n",
    "        ########################################\n",
    "        \n",
    "        # 1 - click on the bottom of the chat after it has loaded\n",
    "        self.supp_wait_for_xpath(messages_xp, \"at_least_one_message_in_chat\")\n",
    "        all_messages = self.driver.find_elements_by_xpath(messages_xp)\n",
    "        topMessage_ID_old = all_messages[0].id\n",
    "        ActionChains(self.driver).move_to_element(all_messages[-1]).click().perform()\n",
    "\n",
    "\n",
    "        # 2 - prepare variables for scraping\n",
    "        scraped_messages = []           # cumulative array of all the scraped_messages\n",
    "        oldest_date =  \"Undefined\"\n",
    "        continue_scroll = True\n",
    "        current_number_of_scrolls = 0\n",
    "        scroll_stall = 0            # counter to check how long scrolling has been stalled for\n",
    "        print(\"    \", end=\"\")\n",
    "        \n",
    "        while (continue_scroll):\n",
    "\n",
    "            # 3 - scroll the chat\n",
    "            ActionChains(self.driver).send_keys(Keys.PAGE_UP).perform()\n",
    "\n",
    "            # 4 - get all the scraped_messages in the current scope of the all_messages\n",
    "            all_messages = self.driver.find_elements_by_xpath(messages_xp)\n",
    "            topMessage_new = all_messages[0]\n",
    "            topMessage_ID_new = topMessage_new.id\n",
    "\n",
    "            if(topMessage_ID_new == topMessage_ID_old):\n",
    "                # 5 - if the top of the chat has not updated, jitter the chat by clicking and scrolling up and down\n",
    "                print(\"*\", end = \"\")\n",
    "                ActionChains(self.driver).move_to_element(topMessage_new).click().perform()\n",
    "                scroll_stall += 1\n",
    "                ActionChains(self.driver).send_keys(Keys.PAGE_DOWN).perform()\n",
    "                \n",
    "            else:\n",
    "                # 6 - otherwise continue scrolling\n",
    "                print(\".\", end = \"\")\n",
    "                scroll_stall = 0\n",
    "\n",
    "            # 7 - extract all scraped_messages and reverse the order so thatthey go NEW -> OLD\n",
    "            # (instead of the OLD -> NEW that top down scraping gives)\n",
    "            messages_to_add = self.skype_scrape_chat_visible()\n",
    "            messages_to_add = messages_to_add[::-1]\n",
    "\n",
    "            # 8 - store all scraped_messages with defined dates\n",
    "            for i in messages_to_add:\n",
    "                if(i[1] != \"Undefined\"):\n",
    "                    scraped_messages.append(i)\n",
    "                    oldest_date = i[1]            \n",
    "\n",
    "            # 9 - check if scroll conditions are still satisfied and repeat loop\n",
    "            continue_scroll = self.skype_scrollCheck_date(self.criteria,\n",
    "                                                          {\"current_date\": oldest_date,\n",
    "                                                           \"current_number_of_stalls\": scroll_stall})\n",
    "\n",
    "            # 10 - reset variables next loop\n",
    "            topMessage_ID_old = topMessage_ID_new\n",
    "            current_number_of_scrolls +=1\n",
    "           \n",
    "        # 11 - store the thefiltered content to pandas DataFrame\n",
    "        skype_class.skype_format_messages(scraped_messages, self.criteria)\n",
    "               \n",
    "        # 12 - notify about result of scrolling\n",
    "        filter_target_date = self.criteria['date_max']\n",
    "        # if we happen be without a defined date (early on in the scrolling), define is as today\n",
    "        if(oldest_date != \"Undefined\"):\n",
    "            oldest_date = self.date_from_string(oldest_date)\n",
    "        else:                           \n",
    "            today = datetime.datetime.today()\n",
    "            oldest_date = [today.year, today.month, today.day]\n",
    "        if (self.datetime_from_date(oldest_date) <= self.datetime_from_date(filter_target_date)):\n",
    "            print(\"  âœ” Reached target date: %s\\n\"%(self.string_from_date(filter_target_date)))\n",
    "        else:\n",
    "            print(\"  âœ˜ DID NOT REACH TARGET DATE: %s/%s\\n\"\n",
    "                  %(self.string_from_date(oldest_date), self.string_from_date(filter_target_date).upper()))\n",
    "\n",
    "        return scraped_messages\n",
    "        \n",
    "\n",
    "    def skype_scrape_chat_visible(self):\n",
    "        \"\"\"\n",
    "        __ Description __\n",
    "        goes through the messages visible on the screen and extracts [sender, date, message]\n",
    "\n",
    "        â¦¿â¦¿â¦¿ date is of the form \"09 March 2019\" or \"Monday\" â¦¿â¦¿â¦¿\n",
    "\n",
    "        __ Return __\n",
    "        [1D-(sender, date, message)]  array of tuples holding info on each message.\n",
    "        \"\"\"\n",
    "        ######################################## Attributes and names to identify messages in cconvof\n",
    "        soup_chat_message = [[\"div\", {\"role\": [\"region\", \"heading\"], \"tabindex\": re.compile(\"(-1|0)\"), \"aria-label\": re.compile(\".\")}]]\n",
    "        ########################################\n",
    "\n",
    "        \n",
    "        # 1 - get html of the page and look for messages with beautiful soup\n",
    "        soup = self.supp_load_soup()\n",
    "        chatContent = self.supp_extract_html(soup, soup_chat_message)\n",
    "\n",
    "        messages = []\n",
    "        date_current = \"Undefined\"\n",
    "        \n",
    "        for i in chatContent:\n",
    "            # 2 - extract contents of the messages\n",
    "            message_content = i[\"aria-label\"]\n",
    "\n",
    "            # a - date extraction\n",
    "            date = re.match(\n",
    "                re.compile(\"((\\d{2}\\s(January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{4})|(Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday))\"),\n",
    "                            message_content)\n",
    "            if(date):\n",
    "                date = date.group(1)\n",
    "                date_current = date\n",
    "\n",
    "            # b - sender extraction\n",
    "            #     sender comes before the main part of the message e.g. \"YAU, shall we go ....\"\n",
    "            sender = re.search(re.compile(\"(^\\w+(\\s\\w+)?)(,)\"), message_content)\n",
    "            if(sender):\n",
    "                sender = sender.group(1)\n",
    "            else:\n",
    "                sender = None\n",
    "\n",
    "            # c - message: comes after the sender with a comma and before the time sent e.g. \"yau, SHALL WE GO..., sent at 18:00\"\n",
    "            message = re.search(re.compile(\"(^\\w+(\\s\\w+)?,)(.*)(, sent at \\d{2}:\\d{2})\"), message_content)\n",
    "            if(message):\n",
    "                message = message.group(3)\n",
    "            else:\n",
    "                message = None\n",
    "\n",
    "            # 2 - store the message if it was NOT a date (e.g. 09 March 2019)\n",
    "            if((not date) and message):\n",
    "                messages.append((sender, date_current, message))\n",
    "\n",
    "        return messages\n",
    "\n",
    "    def skype_format_messages(self, messages_to_format, critetia):\n",
    "        \"\"\"\n",
    "        __ Parameters __\n",
    "        [arr] messages_to_format:   list of tuples of the form\n",
    "                                    (sender, dateString, message)\n",
    "\n",
    "        __ Description __\n",
    "        the list of messages is scanned and:\n",
    "        - messages with an unassigned dates (\"Undefined\") are given a date\n",
    "        - duplicate messages are removed\n",
    "        - messages newer than a certain values are removed\n",
    "\n",
    "        __ Return __\n",
    "        [pd.DataFrame] pandas_out: dataFrame with all the messages_to_format\n",
    "        \"\"\"\n",
    "\n",
    "        message_to_format = messages_to_format[::-1] # revese the message order, so that oldest (with defined date) are on top\n",
    "        self.reset()\n",
    "\n",
    "        # setup dates\n",
    "        date_min = criteria['min_date']\n",
    "        date_min = datetime.datetime(date_min[0], date_min[1], date_min[2])\n",
    "        date_max = criteria['max_date']\n",
    "        date_max = datetime.datetime(date_max[0], date_max[1], date_max[2])\n",
    "        running_date = \"Undefined\" # date that keeps track of where we are in the mssages\n",
    "        \n",
    "        # 1 - iterate the messages, resolving any dates that were not extracted during scrolling\n",
    "        for i in messages_to_format:\n",
    "\n",
    "            date = i[1]\n",
    "\n",
    "            if(date == \"Undefined\"):\n",
    "                # a - if a date was not defined, look at the previously defined date\n",
    "                date = running_date\n",
    "\n",
    "            else:\n",
    "                # b - convert date to [year, month, day]\n",
    "                date = self.date_from_string(date)\n",
    "                running_date = date # store the running date, so that further dates can be infered from it\n",
    "                \n",
    "            # 2 - write messages with defined dates that fall in the defined filter region\n",
    "            if(date != \"Undefined\"):\n",
    "\n",
    "                date = self.datetime_from_date(date)\n",
    "                \n",
    "                if((date >= date_min) and (date <= date_max)):\n",
    "                    date_string = date.strftime(\"%d %B %Y\")\n",
    "                    # 2 - store the message in a dataframe\n",
    "                    message_to_store = {\"From\": i[0],\n",
    "                                        \"Date\": date_string,\n",
    "                                        \"Message\": i[2]}\n",
    "\n",
    "                    self.pandas_scraped = self.pandas_scraped.append(message_to_store, ignore_index=True)\n",
    "\n",
    "        # 3 - remove duplicate entries due to scrolling overlap\n",
    "        self.pandas_scraped = self.pandas_scraped.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wait_for_chat_update():\n",
    "  \"\"\"Checking that Skype chat has updated after scrolling has been performed\n",
    "\n",
    "  To be used in the following way:\n",
    "  formWebDriverWait.until(wait_for_chat_update(old_top_message))\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, top_message_text_old):\n",
    "      self.top_message_text_old = top_message_text_old\n",
    "      \n",
    "  def __call__(self, driver):\n",
    "    \"\"\"\n",
    "    __ Description __\n",
    "    compares the id of the top message after scrolling. \n",
    "    if scrolling has stopped (end of conversation of loading) the id will remain the same\n",
    "\n",
    "    __ Return __\n",
    "    True: if text stayed the same - need to perform a click action\n",
    "    False: if text has changed - can continue scrolling\n",
    "    \"\"\"\n",
    "\n",
    "    ########################################xpaths\n",
    "    chatBox_xpath = \"//div[@style='position: relative; display: flex; flex-direction: row; flex-grow: 1; flex-shrink: 1; overflow: hidden; align-items: stretch; background-color: rgb(255, 255, 255);']\"\n",
    "    ########################################\n",
    "    \n",
    "    chatBox = driver.find_element_by_xpath(chatBox_xpath).find_elements_by_xpath(\"//div[@role='region']\")\n",
    "\n",
    "    top_message_text_new = chatBox[0].id\n",
    "    \n",
    "    if(top_message_text_new == self.top_message_text_old):\n",
    "        # print(\"clicked and no change\")\n",
    "        return False\n",
    "    else:\n",
    "        # print(\"clicking has caused content to load\")\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  > Starting new Chrome server\n",
      "==> setup_browser end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "########################################\n",
    "skype_id = \"ðŸ„\"\n",
    "password = \"ðŸ„ðŸ„\"\n",
    "timeout = 10                      # seconds to wait for page elements to load\n",
    "browser = \"chrome\"                # firefox of chrome\n",
    "########################################\n",
    "########################################\n",
    "skype_class = skype_bot(browser, timeout)\n",
    "#skype_class.login(skype_id, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> skype_scrape_setup start\n",
      "  > Maximal number of stalls:\t 14\n",
      "  > Minimum date:\t\t Friday, 01 Feb 2019\n",
      "  > Maximal date:\t\t Saturday, 20 Apr 2019\n",
      "==> skype_scrape_setup end\n",
      "\n",
      "==> skype_scrape start\n"
     ]
    },
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"div/div/div[2]/div[1]/div\"}\n  (Session info: chrome=77.0.3865.90)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-046432ead14f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m########################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mskype_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskype_scrape_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchats_to_extract\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_number_of_stalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mskype_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskype_scrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-561c936696ca>\u001b[0m in \u001b[0;36mskype_scrape\u001b[0;34m(self, ext)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;31m# 3 - get the sender\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0msender\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchats_in_sidebar_sender_xp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data-text-as-pseudo-element\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m                 \u001b[0;31m# replace dots, @ and spaces with underscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0msender\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/creamy_seas/sync_files/python_vi/training/lib/python3.7/site-packages/selenium/webdriver/remote/webelement.py\u001b[0m in \u001b[0;36mfind_element_by_xpath\u001b[0;34m(self, xpath)\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'//div/td[1]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \"\"\"\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_elements_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/creamy_seas/sync_files/python_vi/training/lib/python3.7/site-packages/selenium/webdriver/remote/webelement.py\u001b[0m in \u001b[0;36mfind_element\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m         return self._execute(Command.FIND_CHILD_ELEMENT,\n\u001b[0;32m--> 659\u001b[0;31m                              {\"using\": by, \"value\": value})['value']\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/creamy_seas/sync_files/python_vi/training/lib/python3.7/site-packages/selenium/webdriver/remote/webelement.py\u001b[0m in \u001b[0;36m_execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/creamy_seas/sync_files/python_vi/training/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[0;32m~/creamy_seas/sync_files/python_vi/training/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"div/div/div[2]/div[1]/div\"}\n  (Session info: chrome=77.0.3865.90)\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "########################################\n",
    "chats_to_extract = [0,1]#[0, 1, 2, 4, 6, 10]    # chats to extract, given by index\n",
    "max_number_of_stalls = 14\n",
    "min_date = [2019, 2, 1]        # either None or [2018, 1, 1]\n",
    "max_date = [2019, 4, 20]        # either None or [2018, 1, 1]\n",
    "########################################\n",
    "########################################\n",
    "skype_class.skype_scrape_setup(chats_to_extract, max_number_of_stalls, min_date, max_date)\n",
    "skype_class.skype_scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "name": "Untitled.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
